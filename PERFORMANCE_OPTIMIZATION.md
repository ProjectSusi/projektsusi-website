# Performance Optimization Guide - ProjektSusui RAG System

## ðŸš€ Current Performance Metrics

### Baseline Performance (Single Instance)
- **Query Response Time**: 7-8 seconds (with LLM)
- **Document Upload**: 2-5 seconds per MB
- **Vector Search**: 50-200ms for 10k documents
- **Concurrent Users**: 50-100
- **Memory Usage**: 2-4 GB
- **CPU Usage**: 40-60% under load

## ðŸŽ¯ Optimization Targets

### Target Metrics
- **Query Response Time**: < 2 seconds
- **Document Upload**: < 1 second per MB
- **Vector Search**: < 50ms
- **Concurrent Users**: 500+
- **Memory Usage**: < 3 GB
- **CPU Usage**: < 50% under load

## ðŸ”§ Quick Wins (Immediate Impact)

### 1. Enable Redis Caching
```bash
# Install Redis
sudo apt install redis-server

# Configure in .env
REDIS_URL=redis://localhost:6379
ENABLE_CACHING=true
CACHE_TTL_SECONDS=3600

# Verify caching
curl http://localhost:8000/metrics | grep cache_hit_rate
```

### 2. Database Connection Pooling
```python
# In config/config.py
DATABASE_POOL_SIZE = int(os.getenv('DATABASE_POOL_SIZE', '20'))
DATABASE_MAX_OVERFLOW = int(os.getenv('DATABASE_MAX_OVERFLOW', '40'))
DATABASE_POOL_TIMEOUT = int(os.getenv('DATABASE_POOL_TIMEOUT', '30'))

# In repositories/base.py
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=DATABASE_POOL_SIZE,
    max_overflow=DATABASE_MAX_OVERFLOW,
    pool_timeout=DATABASE_POOL_TIMEOUT,
    pool_pre_ping=True  # Verify connections
)
```

### 3. Optimize Ollama Response Time
```bash
# Use faster models
ollama pull llama3.2:1b  # Ultra-fast, 1B parameters
ollama pull phi3-mini    # Fast, good quality

# Reduce token generation
export OLLAMA_MAX_TOKENS=256
export OLLAMA_TEMPERATURE=0.1
export OLLAMA_TOP_K=20

# Enable GPU if available
export OLLAMA_GPU=true
```

### 4. Enable Response Streaming
```python
# In ollama_client.py
def generate_answer_stream(self, query: str, context: str):
    """Stream response for better perceived performance"""
    payload = {
        "model": self.model,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 256,
            "temperature": 0.1
        }
    }
    
    response = requests.post(
        f"{self.base_url}/api/generate",
        json=payload,
        stream=True
    )
    
    for line in response.iter_lines():
        if line:
            yield json.loads(line)["response"]
```

## ðŸ“ˆ Database Optimizations

### 1. Create Indexes
```sql
-- Essential indexes for performance
CREATE INDEX idx_documents_tenant_created ON documents(tenant_id, created_at DESC);
CREATE INDEX idx_chunks_document_id ON chunks(document_id);
CREATE INDEX idx_chunks_text_pattern ON chunks(text_content varchar_pattern_ops);
CREATE INDEX idx_embeddings_chunk_id ON embeddings(chunk_id);
CREATE INDEX idx_embeddings_vector ON embeddings USING ivfflat (embedding_vector);

-- Analyze tables
VACUUM ANALYZE documents;
VACUUM ANALYZE chunks;
VACUUM ANALYZE embeddings;
```

### 2. Optimize Queries
```python
# Use batch operations
async def bulk_insert_chunks(chunks: List[Chunk]):
    """Insert chunks in batches"""
    await db.execute_many(
        "INSERT INTO chunks VALUES (?, ?, ?, ?)",
        [(c.id, c.document_id, c.text, c.index) for c in chunks]
    )

# Use prepared statements
prepared_stmt = await conn.prepare(
    "SELECT * FROM documents WHERE tenant_id = $1 LIMIT $2"
)
results = await prepared_stmt.fetch(tenant_id, limit)
```

### 3. PostgreSQL Tuning
```ini
# postgresql.conf optimizations
shared_buffers = 256MB          # 25% of RAM
effective_cache_size = 1GB      # 50-75% of RAM
maintenance_work_mem = 64MB
work_mem = 4MB
max_connections = 200
random_page_cost = 1.1          # For SSD
```

## ðŸ” Vector Search Optimization

### 1. FAISS Index Optimization
```python
# Use IVF index for large datasets
import faiss

# For 100k-1M vectors
index = faiss.IndexIVFPQ(
    quantizer,        # Coarse quantizer
    dimension,        # Vector dimension
    nlist=100,       # Number of clusters
    M=8,             # Number of subquantizers
    nbits=8          # Bits per subquantizer
)

# Train index with sample data
index.train(training_vectors)

# Optimize search parameters
index.nprobe = 10  # Number of clusters to search
```

### 2. Dimension Reduction
```python
# Reduce embedding dimensions for speed
from sklearn.decomposition import PCA

# Reduce from 768 to 256 dimensions
pca = PCA(n_components=256)
reduced_embeddings = pca.fit_transform(embeddings)

# 3x faster search with ~5% accuracy loss
```

### 3. Batch Processing
```python
async def batch_vector_search(queries: List[str], batch_size: int = 10):
    """Process vector searches in batches"""
    results = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i+batch_size]
        batch_embeddings = model.encode(batch)
        batch_results = index.search(batch_embeddings, k=5)
        results.extend(batch_results)
    return results
```

## âš¡ Caching Strategies

### 1. Multi-Level Caching
```python
# L1: In-memory cache (fastest)
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_cached_embedding(text: str):
    return model.encode(text)

# L2: Redis cache (shared)
async def get_cached_response(query: str):
    cache_key = f"response:{hashlib.md5(query.encode()).hexdigest()}"
    
    # Check Redis
    cached = await redis.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Generate and cache
    response = await generate_response(query)
    await redis.setex(cache_key, 3600, json.dumps(response))
    return response

# L3: CDN for static assets
# Configure nginx to cache static files
```

### 2. Query Result Caching
```python
class QueryCache:
    def __init__(self, ttl: int = 3600):
        self.cache = {}
        self.ttl = ttl
    
    def get_cache_key(self, query: str, filters: dict) -> str:
        """Generate deterministic cache key"""
        key_data = f"{query}:{json.dumps(filters, sort_keys=True)}"
        return hashlib.sha256(key_data.encode()).hexdigest()
    
    async def get_or_compute(self, query: str, filters: dict, compute_fn):
        cache_key = self.get_cache_key(query, filters)
        
        # Check cache
        if cache_key in self.cache:
            entry = self.cache[cache_key]
            if time.time() - entry['timestamp'] < self.ttl:
                return entry['data']
        
        # Compute and cache
        result = await compute_fn()
        self.cache[cache_key] = {
            'data': result,
            'timestamp': time.time()
        }
        return result
```

## ðŸ”„ Async Processing

### 1. Background Job Queue
```python
# Use Celery for async tasks
from celery import Celery

app = Celery('projektsusui', broker='redis://localhost:6379')

@app.task
def process_document_async(document_id: int):
    """Process document in background"""
    # Heavy processing here
    extract_text()
    generate_embeddings()
    update_index()

# In API endpoint
@router.post("/documents")
async def upload_document(file: UploadFile):
    document_id = await save_document(file)
    process_document_async.delay(document_id)  # Non-blocking
    return {"id": document_id, "status": "processing"}
```

### 2. Concurrent Processing
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=4)

async def process_multiple_queries(queries: List[str]):
    """Process queries concurrently"""
    tasks = []
    for query in queries:
        task = asyncio.create_task(process_query(query))
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results
```

## ðŸŽ® Load Balancing

### 1. Multiple Worker Processes
```bash
# Gunicorn with multiple workers
gunicorn core.main:app \
    --workers 4 \
    --worker-class uvicorn.workers.UvicornWorker \
    --bind 0.0.0.0:8000 \
    --preload \
    --max-requests 1000 \
    --max-requests-jitter 50
```

### 2. Nginx Load Balancing
```nginx
upstream app_servers {
    least_conn;  # Or ip_hash for session persistence
    server 127.0.0.1:8000 weight=1;
    server 127.0.0.1:8001 weight=1;
    server 127.0.0.1:8002 weight=1;
    keepalive 32;
}

server {
    location /api {
        proxy_pass http://app_servers;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
}
```

## ðŸ“Š Monitoring & Profiling

### 1. Performance Profiling
```python
import cProfile
import pstats

def profile_function(func):
    """Decorator for profiling functions"""
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()
        result = func(*args, **kwargs)
        profiler.disable()
        
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
        stats.print_stats(10)  # Top 10 time consumers
        
        return result
    return wrapper

@profile_function
def slow_function():
    # Function to profile
    pass
```

### 2. APM Integration
```python
# DataDog APM
from ddtrace import tracer

@tracer.wrap()
async def traced_function():
    """Function with APM tracing"""
    with tracer.trace("database.query"):
        result = await db.query()
    return result

# Prometheus metrics
from prometheus_client import Histogram

query_duration = Histogram(
    'query_duration_seconds',
    'Query processing duration'
)

@query_duration.time()
async def process_query(query: str):
    # Query processing
    pass
```

## ðŸš„ Performance Testing

### 1. Load Testing Script
```python
# locustfile.py
from locust import HttpUser, task, between

class RAGUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)
    def search_query(self):
        self.client.post("/api/v1/query", json={
            "query": "What is the rental price?",
            "top_k": 5
        })
    
    @task(1)
    def upload_document(self):
        with open("test.pdf", "rb") as f:
            self.client.post("/api/v1/documents",
                files={"file": f}
            )

# Run: locust -f locustfile.py --host=http://localhost:8000
```

### 2. Benchmark Results
```bash
# Apache Bench for simple tests
ab -n 1000 -c 10 -T application/json \
   -p query.json \
   http://localhost:8000/api/v1/query

# Expected results after optimization:
# Requests per second: 100+ [#/sec]
# Time per request: <100ms (50th percentile)
# Time per request: <500ms (99th percentile)
```

## ðŸŽ¯ Optimization Checklist

### Immediate (1 Day)
- [ ] Enable Redis caching
- [ ] Implement connection pooling
- [ ] Add database indexes
- [ ] Use faster Ollama model
- [ ] Enable response streaming

### Short-term (1 Week)
- [ ] Implement background job queue
- [ ] Add query result caching
- [ ] Optimize vector search index
- [ ] Setup load balancer
- [ ] Add monitoring

### Medium-term (1 Month)
- [ ] Implement dimension reduction
- [ ] Add CDN for static assets
- [ ] Optimize database queries
- [ ] Implement auto-scaling
- [ ] Add APM integration

### Long-term (3 Months)
- [ ] Migrate to GPU inference
- [ ] Implement distributed caching
- [ ] Add read replicas
- [ ] Implement sharding
- [ ] Optimize for specific hardware

## ðŸ“ˆ Expected Performance Gains

| Optimization | Performance Gain | Implementation Effort |
|-------------|-----------------|----------------------|
| Redis Caching | 50-70% faster | Low |
| Connection Pooling | 20-30% faster | Low |
| Database Indexes | 40-60% faster | Low |
| Faster LLM Model | 60-80% faster | Low |
| Response Streaming | Better UX | Medium |
| Background Jobs | 10x for uploads | Medium |
| Vector Index Optimization | 3-5x faster | High |
| Load Balancing | 3-4x throughput | Medium |
| GPU Inference | 10x faster | High |

## ðŸ”¥ Performance Hotspots

Based on profiling, the main bottlenecks are:

1. **LLM Generation** (60% of response time)
   - Solution: Faster models, caching, streaming

2. **Vector Search** (20% of response time)
   - Solution: Better indexing, dimension reduction

3. **Database Queries** (10% of response time)
   - Solution: Indexes, connection pooling

4. **Text Processing** (5% of response time)
   - Solution: Async processing, caching

5. **Network I/O** (5% of response time)
   - Solution: Connection reuse, compression

## ðŸ’¡ Pro Tips

1. **Use the 80/20 rule**: Focus on optimizations that give 80% improvement with 20% effort
2. **Measure before optimizing**: Always profile to find real bottlenecks
3. **Cache aggressively**: Cache at every level possible
4. **Optimize for common cases**: Focus on frequently used queries
5. **Monitor continuously**: Set up alerts for performance degradation

---
*Remember: Premature optimization is the root of all evil. Profile first, optimize second.*